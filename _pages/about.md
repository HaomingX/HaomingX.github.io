---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## About Me

I am Haoming Xu, a Master's student in the College of Software at Zhejiang University, focusing on large language models and natural language processing.

My research interests center on **editing, unlearning** and **continual learning** for large language models. I work on developing methods to remove specific knowledge from LLMs while preserving performance, as well as exploring how models can learn continuously and achieve evolution. I also maintain an active interest in model reliability, hallucination mitigation, and broader topics in AI safety and robustness.

I am seeking collaboration peers with same research insterests.

If you are interested in my work, please contact me at **haomingxu2003@gmail.com**.

---

## üìù Publications {#publications}

*You can find my publications on [Google Scholar](https://scholar.google.com.hk/citations?user=I_sHcmgAAAAJ&hl=zh-CN).*

<div class="publication-year">2026</div>

<div class="publication-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://arxiv.org/abs/2601.05905">Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://arxiv.org/abs/2601.05905" class="card-meta-icon" title="arXiv"><i class="fas fa-file-pdf"></i></a>
          <a href="https://github.com/zjunlp/belief" class="card-meta-icon" title="Code"><i class="fab fa-github"></i></a>
        </span>
        <span>arXiv 2026</span>
      </div>
      <div class="card-authors"><strong>Haoming Xu</strong>, Ningyuan Zhao, Yunzhi Yao, Weihong Xu, Hongru Wang, Xinle Deng, Shumin Deng, Jeff Z. Pan, Huajun Chen, Ningyu Zhang</div>
      <div class="card-description">We propose a novel framework for diagnosing LLM truthfulness via neighborhood consistency analysis.</div>
    </div>
  </div>
</div>

<div class="publication-year">2025</div>

<div class="publication-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://www.authorea.com/users/812306/articles/1234567-rethinking-knowledge-editing-in-reasoning-era">Rethinking Knowledge Editing in Reasoning Era</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://www.authorea.com/users/812306/articles/1234567-rethinking-knowledge-editing-in-reasoning-era" class="card-meta-icon" title="Paper"><i class="fas fa-file-pdf"></i></a>
        </span>
        <span>Authorea 2025</span>
      </div>
      <div class="card-authors">Yunzhi Yao, Jiaxin Qin, Ningyu Zhang, <strong>Haoming Xu</strong>, Yuqi Zhu, Zeping Yu, Mengru Wang, Yuqi Tang, Jia-Chen Gu, Shumin Deng, Nanyun Peng, Huajun Chen</div>
      <div class="card-description">Revisiting knowledge editing methods for large language models in the reasoning era.</div>
    </div>
  </div>
</div>

<div class="publication-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://arxiv.org/abs/2510.18866">LightMem: Lightweight and Efficient Memory-Augmented Generation</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://arxiv.org/abs/2510.18866" class="card-meta-icon" title="Paper"><i class="fas fa-file-pdf"></i></a>
          <a href="https://github.com/zjunlp/LightMem" class="card-meta-icon" title="Code"><i class="fab fa-github"></i></a>
        </span>
        <span>arXiv 2025</span>
      </div>
      <div class="card-authors">Jizhan Fang, Xinle Deng, <strong>Haoming Xu</strong>, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
      <div class="card-description">A lightweight and efficient memory-augmented generation framework for large language models.</div>
    </div>
  </div>
</div>

<div class="publication-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://arxiv.org/abs/2504.15133">EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://arxiv.org/abs/2504.15133" class="card-meta-icon" title="Paper"><i class="fas fa-file-pdf"></i></a>
          <a href="https://github.com/zjunlp/EasyEdit" class="card-meta-icon" title="Code"><i class="fab fa-github"></i></a>
        </span>
        <span>EMNLP 2025 Demo Track</span>
      </div>
      <div class="card-authors">Ziwen Xu, Shuxun Wang, Kewei Xu, <strong>Haoming Xu</strong>, Mengru Wang, Xinle Deng, Yunzhi Yao, Guozhou Zheng, Huajun Chen, Ningyu Zhang</div>
      <div class="card-description">An easy-to-use steering framework for editing large language models with improved performance.</div>
    </div>
  </div>
</div>

<div class="publication-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://arxiv.org/abs/2503.21088">ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://arxiv.org/abs/2503.21088" class="card-meta-icon" title="Paper"><i class="fas fa-file-pdf"></i></a>
          <a href="https://github.com/zjunlp/unlearn/tree/main/semeval25" class="card-meta-icon" title="Code"><i class="fab fa-github"></i></a>
        </span>
        <span>ACL2025 Workshop - SemEval</span>
      </div>
      <div class="card-authors"><strong>Haoming Xu</strong>, Shuxun Wang, Yanqiu Zhao, Yi Zhong, Ziyan Jiang, Ningyuan Zhao, Shumin Deng, Huajun Chen, Ningyu Zhang</div>
      <div class="card-description">We propose an unlearning approach via model merging for SemEval-2025 Task 4. We achieved an excellent <strong>second-place</strong> finish.</div>
    </div>
  </div>
</div>

<div class="publication-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://arxiv.org/abs/2502.11190">Relearn: Unlearning via Learning for Large Language Models</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://arxiv.org/abs/2502.11190" class="card-meta-icon" title="Paper"><i class="fas fa-file-pdf"></i></a>
          <a href="https://github.com/zjunlp/unlearn" class="card-meta-icon" title="Code"><i class="fab fa-github"></i></a>
        </span>
        <span>ACL2025 Main Poster</span>
      </div>
      <div class="card-authors"><strong>Haoming Xu</strong>, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang</div>
      <div class="card-description">A novel framework for machine unlearning in large language models through a learning-based approach.</div>
    </div>
  </div>
</div>

<div class="publication-year">2024</div>

<div class="publication-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://arxiv.org/abs/2410.11779">MLLM Can See? Dynamic Correction Decoding for Hallucination Mitigation</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://arxiv.org/abs/2410.11779" class="card-meta-icon" title="Paper"><i class="fas fa-file-pdf"></i></a>
          <a href="https://github.com/zjunlp/DeCo" class="card-meta-icon" title="Code"><i class="fab fa-github"></i></a>
        </span>
        <span>ICLR2025 Poster</span>
      </div>
      <div class="card-authors">Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, <strong>Haoming Xu</strong>, Shumin Deng, Huajun Chen</div>
      <div class="card-description">A dynamic correction decoding strategy for multimodal large language models to mitigate hallucinations.</div>
    </div>
  </div>
</div>

<div class="publication-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://ieeexplore.ieee.org/document/10599305">CMCOQA: A Chinese Medical Complex Open-Question Answering Benchmark</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://ieeexplore.ieee.org/document/10599305" class="card-meta-icon" title="Paper"><i class="fas fa-file-pdf"></i></a>
        </span>
        <span>BIBM 2024</span>
      </div>
      <div class="card-authors"><strong>Zijian Li</strong>, Sendong Zhao, Haochun Wang, <strong>Haoming Xu</strong>, Bing Qin, Ting Liu</div>
      <div class="card-description">A comprehensive benchmark for Chinese medical complex open-question answering systems.</div>
    </div>
  </div>
</div>

<div class="publication-year">2023</div>

<div class="publication-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://arxiv.org/abs/2309.04175">Knowledge-Tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://arxiv.org/abs/2309.04175" class="card-meta-icon" title="Paper"><i class="fas fa-file-pdf"></i></a>
        </span>
        <span>arXiv 2023</span>
      </div>
      <div class="card-authors">Haochun Wang, Sendong Zhao, Zewen Qiang, Zijian Li, Nuwa Xi, Yanrui Du, MuZhen Cai, Haoqiang Guo, Yuhan Chen, <strong>Haoming Xu</strong>, Bing Qin, Ting Liu</div>
      <div class="card-description">Integrating structured medical knowledge bases to improve reliable response generation in Chinese medical QA systems.</div>
    </div>
  </div>
</div>

---

## üöÄ Projects {#projects}

<div class="project-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://github.com/zjunlp/EasyEdit/blob/main/README_2.md">EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://github.com/zjunlp/EasyEdit/blob/main/README_2.md" class="card-meta-icon" title="GitHub"><i class="fab fa-github"></i></a>
        </span>
      </div>
      <div class="card-description">An easy-to-use steering framework for large language models with improved editing capabilities and user experience.</div>
    </div>
  </div>
</div>

<div class="project-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://github.com/zjunlp/EasyEdit">EasyEdit: An Easy-to-use Knowledge Editing Framework for LLMs</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://github.com/zjunlp/EasyEdit" class="card-meta-icon" title="GitHub"><i class="fab fa-github"></i></a>
        </span>
      </div>
      <div class="card-description">An easy-to-use knowledge editing framework for large language models. Supports multiple editing methods including ROME, MEMIT, MEND, and more.</div>
    </div>
  </div>
</div>

<div class="project-card">
  <div class="card-content">
    <div class="card-body">
      <div class="card-title"><a href="https://github.com/zjunlp/LightMem">LightMem: Lightweight and Efficient Memory-Augmented Generation</a></div>
      <div class="card-meta">
        <span class="card-meta-icons">
          <a href="https://github.com/zjunlp/LightMem" class="card-meta-icon" title="GitHub"><i class="fab fa-github"></i></a>
          <a href="https://arxiv.org/abs/2510.18866" class="card-meta-icon" title="Paper"><i class="fas fa-file-pdf"></i></a>
        </span>
      </div>
      <div class="card-description">Lightweight and efficient memory-augmented generation framework for large language models.</div>
    </div>
  </div>
</div>

---

## üìñ Education {#education}

**Master of Software Engineering**  
Zhejiang University, 2025 - present

**Bachelor of Computer Science**  
  Harbin Institute of Technology, 2021 - 2025

---

## üíª Internship Experience {#internships}

**Research Intern** | *July 2023 - June 2024* | Research Center for [Social Computing and Interactive Robotics (SCIR)](https://ir.hit.edu.cn/), Harbin Institute of Technology, Harbin, China