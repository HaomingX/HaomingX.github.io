---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## About Me

I am Haoming Xu, a Master's student in the College of Software at Zhejiang University, focusing on large language models and natural language processing.

My research interests center on **editing, unlearning** and **continual learning** for large language models. I work on developing methods to remove specific knowledge from LLMs while preserving performance, as well as exploring how models can learn continuously and achieve evolution. I also maintain an active interest in model reliability, hallucination mitigation, and broader topics in AI safety and robustness.

I am seeking collaborations with research-minded peers (undergraduates and master's students preparing for further study are welcome) and with fellow researchers.

If you are interested in my work, please contact me at **haomingxu2003@gmail.com**.

You can find my publications on [Google Scholar](https://scholar.google.com.hk/citations?user=I_sHcmgAAAAJ&hl=zh-CN).

## ğŸ“ Publications {#publications}

<!-- Placeholder for publications - fill in your publication details here -->
<!-- Example format:
### Conference Papers

#### Paper Title
**Authors**, Conference Name Year  
*Brief description or excerpt*

[Paper] | [Code] | [Slides]
-->

## ğŸš€ Projects {#projects}

<!-- Placeholder for projects - fill in your project details here -->
<!-- Example format:
### Project Title
*Project description and key contributions*

**Technologies:** Technology 1, Technology 2  
**Links:** [Paper] | [Code] | [Website]
-->

## Education

- **Master of Software Engineering**  
  Zhejiang University, 2025 - x

- **Bachelor of Computer Science**  
  Harbin Institute of Technology, 2021 - 2025

## ğŸ’» Internship Experience {#internships}

<!-- Update this section with your internship details -->
<!-- Example format:
**Research Intern** | *July 2024 - Present* | Organization Name, Location

**Primary Responsibilities:** Description of main responsibilities

**Key Contributions:**
- Contribution 1
- Contribution 2
-->